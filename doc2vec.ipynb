{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"doc2vec.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python [Root]","language":"python","name":"Python [Root]"}},"cells":[{"metadata":{"id":"1K5r4foOZlnb","colab_type":"code","colab":{}},"cell_type":"code","source":["#https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python/\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SCfEni0YZlnj","colab_type":"code","colab":{}},"cell_type":"code","source":["data = '/Users/davidleifer/Desktop/grant_work/working/twitter-code/user-tweets/TwitterEnergyData/json-data/energy20170801T140040.json'\n","df = pd.read_json(data, lines=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vZuL69aJZlnm","colab_type":"code","colab":{}},"cell_type":"code","source":["from nltk.tokenize import TweetTokenizer\n","from nltk.corpus import stopwords\n","import re, string\n","import nltk\n","tweets_texts = df[\"text\"].tolist()\n","stopwords=stopwords.words('english')\n","english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n","def process_tweet_text(tweet):\n","    if tweet.startswith('@null'):\n","        return \"[Tweet not available]\"\n","    tweet = re.sub(r'\\$\\w*','',tweet) # Remove tickers\n","    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*','',tweet) # Remove hyperlinks\n","    tweet = re.sub(r'['+string.punctuation+']+', ' ',tweet) # Remove puncutations like 's\n","    twtok = TweetTokenizer(strip_handles=True, reduce_len=True)\n","    tokens = twtok.tokenize(tweet)\n","    tokens = [i.lower() for i in tokens if i not in stopwords and len(i) > 2 and  \n","                                             i in english_vocab]\n","    return tokens\n","words = []\n","for tw in tweets_texts:\n","    words += process_tweet_text(tw)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l2M95vuzZlnp","colab_type":"code","colab":{},"outputId":"5b38c2a6-4156-4326-e9fa-3633e8ac4c8c"},"cell_type":"code","source":["from nltk.collocations import *\n","bigram_measures = nltk.collocations.BigramAssocMeasures()\n","finder = BigramCollocationFinder.from_words(words, 5)\n","finder.apply_freq_filter(5)\n","print(finder.nbest(bigram_measures.likelihood_ratio, 10))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[('selection', 'ready'), ('ready', 'solar'), ('canvas', 'art'), ('art', 'painting'), ('canvas', 'painting'), ('type', 'contact'), ('fix', 'type'), ('solar', 'selection'), ('ready', 'selection'), ('selection', 'selection')]\n"],"name":"stdout"}]},{"metadata":{"id":"ro6MXpAmZlnu","colab_type":"code","colab":{}},"cell_type":"code","source":["cleaned_tweets = []\n","for tw in tweets_texts:\n","    words = process_tweet_text(tw)\n","    cleaned_tweet = \" \".join(w for w in words if len(w) > 2 and \n","w.isalpha()) #Form sentences of processed words\n","    cleaned_tweets.append(cleaned_tweet)\n","df['CleanTweetText'] = cleaned_tweets"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uJ4cins-Zlny","colab_type":"code","colab":{},"outputId":"e74f8c09-9f42-4fe4-aa4e-7d5b6777eb7c"},"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer  \n","tfidf_vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,3))  \n","tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_tweets)  \n","feature_names = tfidf_vectorizer.get_feature_names() # num phrases  \n","from sklearn.metrics.pairwise import cosine_similarity  \n","dist = 1 - cosine_similarity(tfidf_matrix)  \n","print(dist) \n","\n","from sklearn.cluster import KMeans\n","num_clusters = 3  \n","km = KMeans(n_clusters=num_clusters)  \n","km.fit(tfidf_matrix)  \n","clusters = km.labels_.tolist()  \n","df['ClusterID'] = clusters  \n","print(df['ClusterID'].value_counts())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[ -2.22044605e-16   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n","    1.00000000e+00   1.00000000e+00]\n"," [  1.00000000e+00   0.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n","    1.00000000e+00   1.00000000e+00]\n"," [  1.00000000e+00   1.00000000e+00  -2.22044605e-16 ...,   1.00000000e+00\n","    1.00000000e+00   1.00000000e+00]\n"," ..., \n"," [  1.00000000e+00   1.00000000e+00   1.00000000e+00 ...,   0.00000000e+00\n","    1.00000000e+00   1.00000000e+00]\n"," [  1.00000000e+00   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n","   -2.22044605e-16   1.00000000e+00]\n"," [  1.00000000e+00   1.00000000e+00   1.00000000e+00 ...,   1.00000000e+00\n","    1.00000000e+00   1.00000000e+00]]\n","1    9310\n","2     449\n","0     242\n","Name: ClusterID, dtype: int64\n"],"name":"stdout"}]},{"metadata":{"id":"v76LFtxZZln1","colab_type":"code","colab":{},"outputId":"94989f59-4f19-40a0-db7a-63b49dd9dcc1"},"cell_type":"code","source":["order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n","for i in range(num_clusters):\n","    print(\"Cluster {} : Words :\".format(i))\n","    for ind in order_centroids[i, :10]: \n","        print(' %s' % feature_names[ind])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cluster 0 : Words :\n"," canvas art painting\n"," canvas art\n"," art painting\n"," canvas\n"," painting\n"," art\n"," fresh edition canada\n"," friendly sector role\n"," fresh edition\n"," fruity perfect\n","Cluster 1 : Words :\n"," solar\n"," oil\n"," nuclear\n"," coal\n"," energy\n"," via\n"," latest\n"," renewable\n"," power\n"," fix\n","Cluster 2 : Words :\n"," selection ready solar\n"," ready solar\n"," selection ready\n"," selection\n"," ready\n"," solar\n"," zone oil\n"," friendly little\n"," friendly little spa\n"," friendly little yellow\n"],"name":"stdout"}]},{"metadata":{"id":"OJqU9tyMZln5","colab_type":"code","colab":{},"outputId":"54fd8fec-ff38-4891-9c5f-01cc06b8a7f7"},"cell_type":"code","source":["from gensim import corpora, models\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import string\n","stop = set(stopwords.words('english'))\n","exclude = set(string.punctuation)\n","lemma = WordNetLemmatizer()\n","\n","def clean(doc):\n","    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n","    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n","    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n","    return normalized\n","\n","texts = [text for text in cleaned_tweets if len(text) > 2]\n","doc_clean = [clean(doc).split() for doc in texts]\n","dictionary = corpora.Dictionary(doc_clean)\n","doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n","ldamodel = models.ldamodel.LdaModel(doc_term_matrix, num_topics=6, id2word = \n","dictionary, passes=5)\n","\n","for topic in ldamodel.show_topics(num_topics=6, formatted=False, num_words=6):\n","    print(\"Topic {}: Words: \".format(topic[0]))\n","    topicwords = [w for (w, val) in topic[1]]\n","    print(topicwords)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using Theano backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Topic 0: Words: \n","['energy', 'renewable', 'via', 'nuclear', 'oil', 'future']\n","Topic 1: Words: \n","['nuclear', 'oil', 'latest', 'energy', 'thorium', 'climate']\n","Topic 2: Words: \n","['fix', 'contact', 'type', 'oil', 'painting', 'art']\n","Topic 3: Words: \n","['solar', 'ready', 'selection', 'path', 'pipeline', 'hydrogen']\n","Topic 4: Words: \n","['solar', 'coal', 'plant', 'power', 'back', 'wind']\n","Topic 5: Words: \n","['coal', 'mine', 'solar', 'oil', 'mining', 'company']\n"],"name":"stdout"}]},{"metadata":{"id":"NsnAsi_BZln9","colab_type":"code","colab":{},"outputId":"c6bd08a7-3e19-4a81-e4ed-65cdd3976730"},"cell_type":"code","source":["import gensim\n","from gensim.models.doc2vec import TaggedDocument\n","taggeddocs = []\n","tag2tweetmap = {}\n","for index,i in enumerate(cleaned_tweets):\n","    if len(i) > 2: # Non empty tweets\n","        tag = u'SENT_{:d}'.format(index)\n","        sentence = TaggedDocument(words=gensim.utils.to_unicode(i).split(), \n","tags=[tag])\n","        tag2tweetmap[tag] = i\n","        taggeddocs.append(sentence)\n","        \n","\n","model = gensim.models.Doc2Vec(taggeddocs, dm=0, alpha=0.025, size=20, \n","min_alpha=0.025, min_count=0)\n","for epoch in range(60):\n","    if epoch % 20 == 0:\n","        print('Now training epoch %s' % epoch)\n","    model.train(taggeddocs, total_examples=model.corpus_count, epochs=model.iter)\n","    model.alpha -= 0.002  # decrease the learning rate\n","    model.min_alpha = model.alpha  # fix the learning rate, no decay"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Now training epoch 0\n","Now training epoch 20\n","Now training epoch 40\n"],"name":"stdout"}]},{"metadata":{"id":"5q2xRCpIZln_","colab_type":"code","colab":{},"outputId":"e5ec0301-15dd-4d46-94f1-c228960d9f34"},"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","dataSet = model.wv.syn0\n","kmeansClustering = KMeans(n_clusters=6)\n","centroidIndx = kmeansClustering.fit_predict(dataSet)\n","topic2wordsmap = {}\n","for i, val in enumerate(dataSet):\n","    tag = model.docvecs.index_to_doctag(i)\n","    topic = centroidIndx[i]\n","    if topic in topic2wordsmap.keys():\n","        for w in (tag2tweetmap[tag].split()):\n","            topic2wordsmap[topic].append(w)\n","    else:\n","        topic2wordsmap[topic] = []\n","for i in topic2wordsmap:\n","    words = topic2wordsmap[i]\n","    print(\"Topic {} has words {}\".format(i, words[:5]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Topic 0 has words ['nuclear', 'oil', 'brent', 'first', 'try']\n","Topic 1 has words ['solar', 'would', 'buyer', 'state', 'vermont']\n","Topic 2 has words ['state', 'except', 'least', 'one', 'plant']\n","Topic 3 has words ['try', 'like', 'happen', 'every', 'time']\n","Topic 4 has words ['semiconducting', 'ink', 'capable', 'hydrogen', 'water']\n","Topic 5 has words ['world', 'potential', 'power', 'energy', 'nuclear']\n"],"name":"stdout"}]},{"metadata":{"id":"8o_q_dzVZloD","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}