{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Twitter stream and harvesting ambient geospatial data\n",
    "\n",
    "## By: David Leifer\n",
    "\n",
    "Following the steps in this documentation will install the needed development environment to do two things. Firstly, you will be able to scrape Twitter data and return the data in JSON text files. Secondly, you will be able to send the JSON text files to OpenStreetMap’s free and open source geocoding application Nominatim. This is tested on a 2015 Apple MacBook Pro Retina, macOS Mojave Version 10.14.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: THE STREAMING SCRIPT\n",
    "\n",
    "1.) Download Anaconda and Jupyter Notebook\n",
    "This assumes that you have at least Python 3.5 installed on you Mac (which it should be automatically).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![anaconda](anaconda.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://www.anaconda.com/distribution/. Click on macOS (your operating system) and download the Python 3.7 version 64-Bit Graphical Installer. You need at least 652.7 MB of space to do this. Double click the .pkg that you just downloaded and click “Continue” three times and then click “Agree” to the terms. Click “Install”. This takes a few minutes.\n",
    "\n",
    "![ana install](ana_install.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this has been installed, you will be asked to install Microsoft Visual Studio Code. This is an IDE for development but I never use it. Skip it for now by pressing “Continue”. Press “Close” and “Move to Trash” for the Anaconda installer package. Verify that it has been correctly installed by typing in “Anaconda- Navigator” in the Spotlight search bar on Mac and clicking the icon (Below Figure).\n",
    "\n",
    "![ana nav](ana_nav.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click “Launch” on the Jupyter Notebook icon. This should launch Jupyter Notebook in your internet web browser.\n",
    "\n",
    "![ana nav2](ana_nav2.png \"Title\")\n",
    "![ana ju_note](ju_note.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Installing the Python 3 Libraries\n",
    "\n",
    "Now on to installing the Python libraries. First, open up Spotlight again and enter “Terminal”. Open up the “Terminal” application and type in “pip install tweepy”. This will download and install the Python library “tweepy” for Python 3.\n",
    "\n",
    "![cl](cl.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure this has installed correctly, open the Jupyter Notebook and create a new Python3 notebook. Open the notebook and copy the following into the first cell and execute the cell:\n",
    "\n",
    "from tweepy.streaming import StreamListener\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "from tweepy import Stream\n",
    "\n",
    "\n",
    "If the cell executes, the libraries were correctly installed.\n",
    "\n",
    "Open up the “energyhashtag1w.py” file with a text editor such as “Sublime Text” or a similar program (if you don’t have “Sublime Text” installation instructions are found here: https://www.sublimetext.com/3). Hover over the variable named “python_program” and change the path to the location of the file. To do this, press “option/command/c” while the file is selected in Finder. Paste the location into the variable. Repeat this step for the second file named “energyhashtag2w.py” for the variable named “the_other_python_script”. Then, repeat this variable path changing for the second file named “energyhashtag2w.py”. These steps are essential to keep the script from crashing.\n",
    "\n",
    "![energy.py](energy.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see if this works by opening up the application “Terminal”. Change Directories to the one with your “energyhashtag1w.py” by entering in: \n",
    "\n",
    "cd /Volumes/Untitled/Desktop/grant_work/jessica_folder/code2/energyhashtag1w.py \n",
    "\n",
    "The second part of the cd will be your path to your file, so make sure to change it.\n",
    "\n",
    "You need to register your application with Twitter developer. Here is a guide to do so:\n",
    "\n",
    "https://docs.inboundnow.com/guide/create-twitter-application/\n",
    "\n",
    "Then, run “python energyhashtag1w.py” to see if the script runs correctly. It should create two files: one ending with .txt and the other with .log. The .txt file is the file with the .json data contained in it. Congratulations, the streamer is now collecting data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is the same as the other one\n",
    "#   except for the access_ and consumer_ keys and tokens\n",
    "# Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "python_program = '/Volumes/Untitled/Desktop/grant_work/jessica_folder/code2/energyhashtag1w.py'\n",
    "the_other_python_script = '/Volumes/Untitled/Desktop/grant_work/jessica_folder/code2/energyhashtag2w.py'\n",
    "\n",
    "logfilename = 'energy' + dt.now().strftime(\"%Y%m%d%H%M%S\") + '.log'\n",
    "logging.basicConfig(filename=logfilename, format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"XXXX\"\n",
    "access_token_secret = \"XXX\"\n",
    "consumer_key = \"XXX\"\n",
    "consumer_secret = \"XXX\"\n",
    "\n",
    "\n",
    "# This is a basic listener that writes received tweets to files.\n",
    "# Each file contains max 10000 tweets\n",
    "class MyListener(StreamListener):\n",
    "    MAINFILENAME = 'energy'\n",
    "    MAXTWEETSINFILE = 10000\n",
    "\n",
    "    def __init__(self, api=None):\n",
    "        self.api = api\n",
    "        super(StreamListener, self).__init__()\n",
    "        self.tweets_count = 0\n",
    "        self.current_file = self.get_file()\n",
    "        self.previous_file = self.current_file\n",
    "        self.rate_limit_exceeded = False\n",
    "        \n",
    "    def on_data(self, data):\n",
    "        self.tweets_count += 1\n",
    "        self.current_file.write(str(data))\n",
    "        if self.tweets_count > MyListener.MAXTWEETSINFILE:\n",
    "            self.previous_file.close()\n",
    "            self.current_file = self.get_file()\n",
    "            self.tweets_count = 0\n",
    "        return True\n",
    "\n",
    "    def on_exception(self, exception):\n",
    "        print(\"on_exception in energyhashtag1w.py on \" + dt.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        logging.warn('on_exception in energyhashtag1w.py')\n",
    "        self.running = False\n",
    "        subprocess.call([sys.executable, python_program, the_other_python_script])\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(\"on_error in energyhashtag1w.py on \" + dt.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "        self.current_file.close()\n",
    "        if status == 420:\n",
    "            logging.warn('rate limit exceeded.')\n",
    "            self.rate_limit_exceeded = True\n",
    "        logging.warn('on_error in energyhashtag1w.py with status code ' + str(status))\n",
    "        self.running = False\n",
    "        subprocess.call([sys.executable, python_program, the_other_python_script])\n",
    "\n",
    "    def get_file(self):\n",
    "        name = MyListener.MAINFILENAME + dt.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "        name += '.txt'\n",
    "        return open(name, 'a')\n",
    "\n",
    "def stop_stream(stream):\n",
    "    stream.listener.stop()\n",
    "    stream.disconnect()\n",
    "    stream.listener.current_file.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    stream = None\n",
    "    try:\n",
    "        #This handles Twitter authetification and the connection to\n",
    "        # Twitter Streaming API\n",
    "        l = MyListener()\n",
    "        auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "        stream = Stream(auth, l)\n",
    "\n",
    "        #This line filter Twitter Streams to capture data by keywords\n",
    "        stream.filter(track=['#gasoline','#Crudeoil', '#crude', '#oil',\n",
    "'#OOTT', '#OPEC', '#algae', '#biodiesel', '#bioethanol', '#Biogas', '#biofuel',\n",
    "'#biofuels', '#ethanol', '#biomass', '#AdvancedBiofuels', '#RFS', '#Fuelcell',\n",
    "'#hydrogen', '#FossilEnergy', '#GreenEnergy', '#thermal', '#coal', '#coalmine',\n",
    "'#geothermal', '#hydroenergy', '#MethaneHydrates', '#biomethane', '#biopower',\n",
    "'#natgas', '#naturalgas', '#SynthesisGas', '#nuclearenergy', '#nuclear',\n",
    "'#OTEC', '#renewable', '#RenewableEnergy', '#TidalEnergy', '#WaveEnergy',\n",
    "'#oceanenergy', '#shalegas', '#solarfarm', '#solar', '#SolarPower',\n",
    "'#SolarEnergy', '#SolarPanels', '#WindEnergy', '#hydropower', '#hydroenergy',\n",
    "'#windfarm', '#WindTurbine', '#RenewableNaturalGas', '#wastetofuel',\n",
    "'#envirofuel', '#wastetoenergy', '#bioenergy', '#syngas', '#cellulosic',\n",
    "'#gasoil', '#graphene', '#LPG', '#cleanenergy', '#Fracking', '#Gasification', '#syngas', '#THE'],stall_warnings=True)\n",
    "    except:\n",
    "        # when exceptions occur, the program did not go here\n",
    "        logging.warning(\"main try except MyException\")\n",
    "        if stream is not None:\n",
    "            stop_stream(stream)\n",
    "        # hard-coded the path of the other script\n",
    "        # os.execv('/home/dxiong/socialmedia/energyhashtag2.py', sys.argv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: THE GEOCODING SCRIPT\n",
    "\n",
    "3.) Installing the Python2 Libraries\n",
    "\n",
    "Up until now, we have been using Python3 through Anaconda to install libraries and execute the streaming script. Since our geocoding script only works with Python2.7, we will need to learn how to switch back and forth.\n",
    "\n",
    "Open up the application “Terminal” and enter “sudo pip2 install pandas”. You will be prompted to enter your password. This should install pandas onto your machine. You will also need to enter the following into “Terminal”:\n",
    "\n",
    "pip2 install ntlk\n",
    "\n",
    "pip2 install json \n",
    "\n",
    "pip2 install glob\n",
    "\n",
    "pip2 install collections\n",
    "\n",
    "pip2 install numpy \n",
    "\n",
    "If you receive an error saying that you do not have a module installed, you did not install the libraries correctly (or I missed one that needs to be installed).\n",
    "\n",
    "Now, to run the script, return to “Terminal” and change directories with “cd” to the place where you store both the “streamlined.py” script and the .txt data you want to geocode. I have included some example .txt files so you can test your script. Run “python2 streamlined.py” and the script will run on the three files, creating a few more files. Congratulations! You can now collect Twitter data and geocode the ambient geospatial information it.\n",
    "\n",
    "Let me know if you have any questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "#S  \t\t\t\tT\t\t\t\t\tA \t\t\t\t\t\tR\t\t\t\t\t\t T#\n",
    "###########################################################################################\n",
    "\n",
    "import json, os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import geocoder\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#for Anaconda python2.7:\n",
    "#source activate py27\n",
    "#data has to be in same dir as the file\n",
    "\n",
    "#path to the current dir\n",
    "#all_files to the dir of txt files\n",
    "# Tweets are stored in in file \"fname\". In the file used for this script, \n",
    "# each tweet was stored on one line\n",
    "#fname = 'data2/energy20180322T083452.txt'\n",
    "path = ''\n",
    "all_files = glob(os.path.join(path, \"*.txt\"))\n",
    "\n",
    "#loop through the all_files dir\n",
    "for fname in all_files:\n",
    "    with open(fname, 'r') as f:\n",
    "        #http://www.mikaelbrunila.fi/2017/03/27/scraping-extracting-mapping-geodata-twitter/\n",
    "        #https://opensas.wordpress.com/2013/06/30/using-openrefine-to-geocode-your-data-using-google-and-openstreetmap-api/\n",
    "        #Create dictionary to later be stored as JSON. All data will be included\n",
    "        # in the list 'data'\n",
    "        users_with_geodata = {\n",
    "            \"data\": []\n",
    "        }\n",
    "        all_users = []\n",
    "        total_tweets = 0\n",
    "        geo_tweets  = 0\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            if tweet['user']['id']:\n",
    "                total_tweets += 1 \n",
    "                user_id = tweet['user']['id']\n",
    "                if user_id not in all_users:\n",
    "                    all_users.append(user_id)\n",
    "                \n",
    "                    #Give users some data to find them by. User_id listed separately \n",
    "                    # to make iterating this data later easier\n",
    "                    user_data = {\n",
    "                        \"user_id\" : tweet['user']['id'],\n",
    "                        \"features\" : {\n",
    "                            \"name\" : tweet['user']['name'],\n",
    "                            \"id\": tweet['user']['id'],\n",
    "                            \"screen_name\": tweet['user']['screen_name'],\n",
    "                            \"tweets\" : 1,\n",
    "                            \"location\": tweet['user']['location'],\n",
    "                            \"text\": tweet['text'],\n",
    "                            \"created_at\": tweet['created_at'],\n",
    "                        }\n",
    "                    }\n",
    "                    #Iterate through different types of geodata to get the variable primary_geo\n",
    "                    if tweet['coordinates']:\n",
    "                        user_data[\"features\"][\"primary_geo\"] = str(tweet['coordinates'][tweet['coordinates'].keys()[1]][1]) + \", \" + str(tweet['coordinates'][tweet['coordinates'].keys()[1]][0])\n",
    "                        user_data[\"features\"][\"geo_type\"] = \"Tweet coordinates\"\n",
    "                    elif tweet['place']:\n",
    "                        user_data[\"features\"][\"primary_geo\"] = tweet['place']['full_name'] + \", \" + tweet['place']['country']\n",
    "                        user_data[\"features\"][\"geo_type\"] = \"Tweet place\"\n",
    "                    else:\n",
    "                        user_data[\"features\"][\"primary_geo\"] = tweet['user']['location']\n",
    "                        user_data[\"features\"][\"geo_type\"] = \"User location\"\n",
    "                    #Add only tweets with some geo data to .json. Comment this if you want to include all tweets.\n",
    "                    if user_data[\"features\"][\"primary_geo\"]:\n",
    "                        users_with_geodata['data'].append(user_data)\n",
    "                        geo_tweets += 1\n",
    "            \n",
    "                #If user already listed, increase their tweet count\n",
    "                elif user_id in all_users:\n",
    "                    for user in users_with_geodata[\"data\"]:\n",
    "                        if user_id == user[\"user_id\"]:\n",
    "                            user[\"features\"][\"tweets\"] += 1\n",
    "    \n",
    "        #Count the total amount of tweets for those users that had geodata            \n",
    "        for user in users_with_geodata[\"data\"]:\n",
    "            geo_tweets = geo_tweets + user[\"features\"][\"tweets\"]\n",
    "        #Get some aggregated numbers on the data\n",
    "        print fname + \" included \" + str(len(all_users)) + \" unique users who tweeted with or without geo data\"\n",
    "        print fname + \" included \" + str(len(users_with_geodata['data'])) + \" unique users who tweeted with geo data, including 'location'\"\n",
    "        print fname + \" users with geo data tweeted \" + str(geo_tweets) + \" out of the total \" + str(total_tweets) + \" of tweets.\"\n",
    "    # Save data to JSON file\n",
    "    with open('user_loc_' + fname + '.json', 'w') as fout:\n",
    "        fout.write(json.dumps(users_with_geodata, indent=4))\n",
    "#create a glob list of the json files in our dir\n",
    "path = ''\n",
    "all_files = glob(os.path.join(path, \"*.json\"))\n",
    "#loop through the glob list of json files\n",
    "for data in all_files:\n",
    "    df = pd.read_json(data)\n",
    "    tweets = pd.read_json((df['data']).to_json(), orient='index')\n",
    "    tweets1 = pd.read_json((tweets['features']).to_json(), orient='index')\n",
    "    tweets1['coord'] = 'coord'\n",
    "\n",
    "    #create a list to append the geo data to, geocode based on location column\n",
    "    for index, row in tweets1.iterrows():\n",
    "        try:\n",
    "        \tprint(row['location'])\n",
    "        \ttime.sleep(1.01)\n",
    "        \tg = geocoder.osm(row['location'])\n",
    "        \tgeo = g.latlng\n",
    "        \tprint(geo)\n",
    "        \ttweets1.at[index, 'coord'] = geo\n",
    "        except:\n",
    "        \tpass\n",
    "\n",
    "    #split the coord column in y and x columns\n",
    "    #tweets1['coord'] = pd.Series(coord)\n",
    "    tweets1['coord'] = tweets1['coord'].astype(str)\n",
    "    tweets1['coord'] = tweets1['coord'].str.strip('[]')\n",
    "    tweets1['y'], tweets1['x'] = tweets1['coord'].str.split(',', 1).str\n",
    "    \n",
    "    #save to a json file\n",
    "    #tweets1.to_json(data + '_geo.json')\n",
    "    print \"Geocoded \" + data\n",
    "    \n",
    "    #sidestep an error reading a string\n",
    "    tweets1 = tweets1[tweets1['text'].notnull()]\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tweets1['tweet_without_stopwords'] = tweets1['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    stop =  ['The','RT','&amp;', '-', 'A', 'https:', '.', '2']\n",
    "    tweets1['tweet_without_stopwords'] = tweets1['tweet_without_stopwords'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #remove periods\n",
    "    tweets1['tweet_without_stopwords'] = tweets1['tweet_without_stopwords'].str.replace('[\\.]','')\n",
    "    #remove commas\n",
    "    tweets1['tweet_without_stopwords'] = tweets1['tweet_without_stopwords'].str.replace('[\\,]','')\n",
    "    #remove -\n",
    "    tweets1['tweet_without_stopwords'] = tweets1['tweet_without_stopwords'].str.replace('[-]','')\n",
    "    #remove @\n",
    "    tweets1['tweet_without_stopwords'] = tweets1['tweet_without_stopwords'].str.replace('[@]','')\n",
    "    \n",
    "    #sentiment analysis using VADER\n",
    "    tweets1[\"compound\"] = ''\n",
    "    tweets1[\"neg\"] = ''\n",
    "    tweets1[\"neu\"] = ''\n",
    "    tweets1[\"pos\"] = ''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for user, row in tweets1.T.iteritems():\n",
    "        try:\n",
    "            sentence = unicodedata.normalize('NFKD', tweets1.loc[user, 'tweet_without_stopwords'])\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            tweets1.set_value(user, 'compound', ss['compound'])\n",
    "            tweets1.set_value(user, 'neg', ss['neg'])\n",
    "            tweets1.set_value(user, 'neu', ss['neu'])\n",
    "            tweets1.set_value(user, 'pos', ss['pos'])\n",
    "        except TypeError:\n",
    "            print(tweets1.loc[user, 'tweet_without_stopwords'])\n",
    "    \n",
    "    #print a positive message and save the file\n",
    "    print \"Sentiment analyzed \" + data\n",
    "    tweets1.to_json(data + '_geo_sent.json')\n",
    "\n",
    "###########################################################################################\n",
    "#F  \t\t\t\t\t\t\t\t\tI \t\t\t\t\t\t\t\t\t\t\t\t N#\n",
    "###########################################################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
